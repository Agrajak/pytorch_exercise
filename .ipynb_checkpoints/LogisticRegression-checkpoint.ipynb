{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pytorch 0.4.1 \n",
    "# https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "# source code from https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py#L33-L34\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn # https://pytorch.org/docs/stable/nn.html \n",
    "import torch.utils as utils\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "# MNIST 에 라벨이 0 to 9이므로 총 10개\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "# for SGD\n",
    "learning_rate = 0.001\n",
    "\n",
    "# pytorch 안에 내장되어있는 MNIST dataset를 불러온다.\n",
    "\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - MNIST: http://yann.lecun.com/exdb/mnist/\n",
    " \n",
    " - Variable : Automatic differentiation package.\n",
    " Variable 클래스는 Tensor를 감싸고 있으며, Tensonr에 정의된 거의 모든 연산을 지원.\n",
    " 모든 계산을 마친 후에 .backward()를 호출하면 자동으로 모든 기울기가 계산된다. 그러나 현재 deprecated. 일반 Tensor(torch.randn(), torch.zeros(), torch.ones()에 require_grad=True)로도 사용가능하다.\n",
    " \n",
    " \n",
    " http://taewan.kim/trans/pytorch/tutorial/blits/02_autograd/\n",
    " https://pytorch.org/docs/stable/autograd.html#variable-deprecated\n",
    "\n",
    " \n",
    " ```python\n",
    "autograd_tensor = torch.randn((2,3,4), requires_grad=True)\n",
    " ```\n",
    " \n",
    " - torch.from_numpy(x)로 numpy로 만든 데이터를 Tensor로 컨버팅 가능하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set:  60000  test_set:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"train_set: \", len(train_set), \" test_set: \", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - nn.Linear :  Applies a linear transformation to the incoming data: \\\\[ y=xA^T+b \\\\] https://pytorch.org/docs/stable/nn.html?highlight=nn%20linear#torch.nn.Linear\n",
    " - torch.Tensor : https://pytorch.org/docs/stable/tensors.html\n",
    " - CrossEntropyLoss() : \n",
    " https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n",
    " combines nn.LogSoftmax() and nn.NLLLoss()(negative log likelihood loss) in one single class\n",
    " \n",
    " \\\\[ loss(x,class)=−log({e^x[class] \\over ∑j e^x[j]}) \\\\]\n",
    " \n",
    " 아니면 좀 더 간단하게\n",
    "```python\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "``` \n",
    "로도 구현가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 2.2381\n",
      "Epoch: [1/5], Step: [200/600], Loss: 2.1738\n",
      "Epoch: [1/5], Step: [300/600], Loss: 2.0742\n",
      "Epoch: [1/5], Step: [400/600], Loss: 1.9650\n",
      "Epoch: [1/5], Step: [500/600], Loss: 1.9123\n",
      "Epoch: [1/5], Step: [600/600], Loss: 1.8137\n",
      "Epoch: [2/5], Step: [100/600], Loss: 1.7538\n",
      "Epoch: [2/5], Step: [200/600], Loss: 1.7071\n",
      "Epoch: [2/5], Step: [300/600], Loss: 1.7373\n",
      "Epoch: [2/5], Step: [400/600], Loss: 1.6277\n",
      "Epoch: [2/5], Step: [500/600], Loss: 1.4991\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.5331\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.4048\n",
      "Epoch: [3/5], Step: [200/600], Loss: 1.4031\n",
      "Epoch: [3/5], Step: [300/600], Loss: 1.2109\n",
      "Epoch: [3/5], Step: [400/600], Loss: 1.3022\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.3035\n",
      "Epoch: [3/5], Step: [600/600], Loss: 1.2172\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.2289\n",
      "Epoch: [4/5], Step: [200/600], Loss: 1.2211\n",
      "Epoch: [4/5], Step: [300/600], Loss: 1.2091\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.1389\n",
      "Epoch: [4/5], Step: [500/600], Loss: 1.1471\n",
      "Epoch: [4/5], Step: [600/600], Loss: 1.1035\n",
      "Epoch: [5/5], Step: [100/600], Loss: 1.0173\n",
      "Epoch: [5/5], Step: [200/600], Loss: 1.1731\n",
      "Epoch: [5/5], Step: [300/600], Loss: 1.0969\n",
      "Epoch: [5/5], Step: [400/600], Loss: 1.0695\n",
      "Epoch: [5/5], Step: [500/600], Loss: 0.9843\n",
      "Epoch: [5/5], Step: [600/600], Loss: 0.9398\n",
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        \n",
    "        \n",
    "        images = images.reshape(-1, input_size)\n",
    "        # the size -1 inferred from other dimention\n",
    "        \n",
    "        \n",
    "        # Variable()은 deprecated므로 사용하지 않는다.\n",
    "        images.requires_grad = True\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                   % (epoch+1, num_epochs, i+1, len(train_set)//batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, input_size))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f5836793ac8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/agrajak/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_SPACE(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_autoscale_on() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-76ac9a351098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_autoscale_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: set_autoscale_on() missing 1 required positional argument: 'b'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#matplot 연습\n",
    "\n",
    "plot_batch_size = 25\n",
    "plot_test_loader = utils.data.DataLoader(test_set, batch_size=plot_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "for images, labels in plot_test_loader:\n",
    "    fig, axs = plt.subplots(nrows=5, ncols=5)\n",
    "    \n",
    "    images = images.reshape(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    images = images.reshape(-1, 28, 28)\n",
    "    cnt = 0\n",
    "    for ax, i in zip(axs.flat, labels):\n",
    "        ax.imshow(images[cnt])\n",
    "        pre = predicted[cnt].data.tolist()\n",
    "        lab = labels[cnt].data.tolist()\n",
    "        ax.set_title(pre)\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        ax.autoscale()\n",
    "        \n",
    "        cnt = cnt+1\n",
    "    \n",
    "    plt.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
